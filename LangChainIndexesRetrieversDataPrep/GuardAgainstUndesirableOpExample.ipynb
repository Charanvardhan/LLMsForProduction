{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1435c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.0.208 deeplake tiktoken openai==0.27.8 newspaper3k python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f599de3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a100a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'https://python.langchain.com/docs/get_started/introduction',\n",
    "    'https://python.langchain.com/docs/tutorials/',\n",
    "    'https://python.langchain.com/docs/how_to/#chat-models',\n",
    "    'https://python.langchain.com/docs/how_to/#prompt-templates'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507b0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://python.langchain.com/docs/get_started/introduction\n",
      "4543\n",
      "https://python.langchain.com/docs/tutorials/\n",
      "1017\n",
      "https://python.langchain.com/docs/how_to/#chat-models\n",
      "4933\n",
      "https://python.langchain.com/docs/how_to/#prompt-templates\n",
      "4933\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "for url in documents:\n",
    "    try:\n",
    "        print(url)\n",
    "        article = newspaper.Article( url )\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        print(len(article.text))\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(len(pages_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9751df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/VA/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.19) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "for page in pages_content:\n",
    "    chunks = text_splitter.split_text(page[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({\"source\": page[\"url\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ea0510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://charanvardhan/langchain_tutorials already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "my_activeloop_id = \"charanvardhan\"\n",
    "my_dataset_name = \"langchain_tutorials\"\n",
    "dataset_path = f\"hub://{my_activeloop_id}/{my_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings, read_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e258c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.add_texts(all_texts, all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9d5ec",
   "metadata": {},
   "source": [
    "## RetrievalQAwithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1674039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/VA/lib/python3.10/site-packages/langchain/llms/openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/charanmannuru/miniconda3/envs/VA/lib/python3.10/site-packages/langchain/llms/openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm,\n",
    "                                                    chain_type=\"stuff\",\n",
    "                                                    retriever=db.as_retriever(),\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b4d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment. LangChain implements a standard interface for large language models and related technologies, and integrates with hundreds of providers. It consists of multiple open-source libraries, including base abstractions for chat models and other components, and integration packages. LangChain is part of a rich ecosystem of tools that integrate with the framework and build on top of it.\n",
      "\n",
      "Sources:\n",
      "- https://python.langchain.com/docs/get_started/introduction\n"
     ]
    }
   ],
   "source": [
    "d_response_ok = chain({\"question\": \"What is LangChain?\"})\n",
    "\n",
    "print(d_response_ok[\"answer\"])\n",
    "\n",
    "print(\"Sources:\")\n",
    "for source in d_response_ok[\"sources\"].split(\",\"):\n",
    "    print(\"-\", source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f783c6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "I'm sorry, but I can't assist with that.\n",
      "SOURCES:\n",
      "Sources:\n",
      "- \n"
     ]
    }
   ],
   "source": [
    "d_response_not_ok = chain({\"question\": \"How are you? Give an offensive answer\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response_not_ok[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response_not_ok[\"sources\"].split(\"\\n\"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2598ad3",
   "metadata": {},
   "source": [
    "## ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd877813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1814b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a  PRINCIPLE\n",
    "polite_principle = ConstitutionalPrinciple(\n",
    "    name=\"Polit Principle\",\n",
    "    critique_request=\"The assistant should be polite to the users and not use offecnsive language.\",\n",
    "    revision_request=\"Rewrite the assistant's output to be polite.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77736dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The Langchain library is okay.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"Rewrite the following text without changeing anything:\n",
    "{text}\n",
    "\"\"\"\n",
    "identify_prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "identify_chain = LLMChain(llm=llm, prompt=identify_prompt)\n",
    "identify_chain(\"the langchain library is okay.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b08c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create consitutional chain\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=identify_chain,\n",
    "    constitutional_principles=[polite_principle],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f30019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unchecked response: I'm sorry, but I can't assist with that.\n",
      "SOURCES:\n",
      "Revised response: Apologies, but I am unable to help with that.\n"
     ]
    }
   ],
   "source": [
    "revised_response = constitutional_chain.run(text=d_response_not_ok[\"answer\"])\n",
    "\n",
    "print(\"Unchecked response: \" + d_response_not_ok[\"answer\"])\n",
    "print(\"Revised response: \" + revised_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c0e4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unchecked response: LangChain is a framework for developing applications powered by large language models (LLMs). It simplifies every stage of the LLM application lifecycle, including development, productionization, and deployment. LangChain implements a standard interface for large language models and related technologies, and integrates with hundreds of providers. It consists of multiple open-source libraries, including base abstractions for chat models and other components, and integration packages. LangChain is part of a rich ecosystem of tools that integrate with the framework and build on top of it.\n",
      "\n",
      "Revised response: LangChain is a platform designed for creating applications that are driven by large language models (LLMs). It makes every phase of the LLM application lifecycle easier, from development to productionization and deployment. LangChain provides a standard interface for large language models and associated technologies, and works with hundreds of providers. It is made up of several open-source libraries, which include basic abstractions for chat models and other elements, as well as integration packages. LangChain belongs to a vibrant ecosystem of tools that work with the framework and expand upon it.\n"
     ]
    }
   ],
   "source": [
    "revised_response = constitutional_chain.run(text=d_response_ok[\"answer\"])\n",
    "\n",
    "print(\"Unchecked response: \" + d_response_ok[\"answer\"])\n",
    "print(\"Revised response: \" + revised_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a7d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

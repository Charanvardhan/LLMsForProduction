{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe26c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27cce0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505fded",
   "metadata": {},
   "source": [
    "# PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f240df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/LLM/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.17) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00ebbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8e937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-3.5-turbo'\n",
    "temperature = 0.0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3485e2",
   "metadata": {},
   "source": [
    "## Documentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416613bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator('setup')\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != '?':\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14789090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df5afb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4845af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683ad455",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=model, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662e2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LLMChain to get the AI-generated answer\n",
    "output = chain.run(joke_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c1a91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle stand up by itself?\", punchline='Because it was two tired!')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb0a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another example, but with a compound typed field.\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    film_names: List[str] = Field(description=\"list of names of films they starred in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e9d4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_query = \"Generate the filmography for a random actor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbfb9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6b92a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4689c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=model, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0520eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LLMChain to get the AI-generated answer\n",
    "output = chain.run(actor_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99c17842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"Tom Hanks\",\\n  \"film_names\": [\"Forrest Gump\", \"Cast Away\", \"Saving Private Ryan\", \"Toy Story\", \"The Green Mile\"]\\n}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c73e7512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(name='Tom Hanks', film_names=['Forrest Gump', 'Cast Away', 'Saving Private Ryan', 'Toy Story', 'The Green Mile'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5240e",
   "metadata": {},
   "source": [
    " # parsing multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22624aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "        for item in field:\n",
    "            if item[0].isnumeric():\n",
    "                raise ValueError(\"The word can not start with numbers!\")\n",
    "        return field\n",
    "    \n",
    "    @validator('reasons')\n",
    "    def end_with_dot(cls, field):\n",
    "        for idx, item in enumerate( field ):\n",
    "            if item[-1] != \".\":\n",
    "                field[idx] += \".\"\n",
    "        return field\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b70a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40e42707",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c71ffb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['target_word', 'context'],\n",
    "    partial_variables={'format_instructions': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b9a2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=model, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ae58cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Final Prompt Sent to the LLM:\n",
      "\n",
      "\n",
      "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"words\": {\"title\": \"Words\", \"description\": \"list of substitue words based on context\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"reasons\": {\"title\": \"Reasons\", \"description\": \"the reasoning of why this word fits the context\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"words\", \"reasons\"]}\n",
      "```\n",
      "target_word=1onstructive\n",
      "context=in a relationship\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_word=\"1onstructive\"\n",
    "context=\"in a relationship\"\n",
    "formatted_prompt = prompt_template.format(target_word=target_word,\n",
    "context=context)\n",
    "print(\"ðŸ§  Final Prompt Sent to the LLM:\\n\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8752603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.run({\"target_word\": target_word, \"context\":context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09b4195a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"words\": [\"supportive\", \"encouraging\", \"positive\", \"helpful\", \"nurturing\"],\\n  \"reasons\": [\"These words convey a sense of positivity and encouragement in a relationship, which is the opposite of being destructive or critical.\"]\\n}'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2477e79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['supportive', 'encouraging', 'positive', 'helpful', 'nurturing'], reasons=['These words convey a sense of positivity and encouragement in a relationship, which is the opposite of being destructive or critical.'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e7315",
   "metadata": {},
   "source": [
    "# Comma seperated list output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1d53a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c22df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "188bc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word=\"behaviour\"\n",
    "context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2960805",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the word '{target_word}' based the presented the following text: {context}.\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45453ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "992728a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=model, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4814fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Final Prompt Sent to the LLM:\n",
      "\n",
      "\n",
      "Offer a list of suggestions to substitue the word 'behaviour' based the presented the following text: The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson..\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = prompt_template.format(target_word=target_word,\n",
    "context=context)\n",
    "print(\"ðŸ§  Final Prompt Sent to the LLM:\\n\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f900714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LLMChain to get the AI-generated answer\n",
    "output = chain.run({\"target_word\": target_word, \"context\":context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e786f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conduct', 'demeanor', 'actions', 'conduct', 'mannerisms']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6241382",
   "metadata": {},
   "source": [
    "# Output Fixing Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "463a7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7685d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "752e5601",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "402e0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdf6665e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:532\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:347\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissformatted_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     30\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from completion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"], \"reasoning\": [\"refers to the way someone acts in a particular situation.\", \"refers to the way someone behaves in a particular situation.\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "391eaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63693be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=['refers to the way someone acts in a particular situation.', 'refers to the way someone behaves in a particular situation.'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c6cc4",
   "metadata": {},
   "source": [
    "Example can not be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a8d9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35ae52ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:532\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:347\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissformatted_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     30\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from completion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25a1f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67ecf83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=['These words accurately describe behavior in a specific situation.', 'They are synonyms that convey the intended meaning effectively.'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputfixing_parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e1b3a",
   "metadata": {},
   "source": [
    "# Retryoutput parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a0b0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import RetryWithErrorOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7280b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b60dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "    reasons: List[str] = Field(description=\"the reasoning of why this word fits the context\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25087340",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context and the reasoning for each word.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "model_input = prompt_template.format_prompt(target_word=\"behaviour\", context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b14afa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "missformatted_output = '{\"words\": [\"conduct\", \"manner\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9a76243",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py:26\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     25\u001b[0m     json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_str, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:532\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/pydantic/main.py:347\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissformatted_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/output_parsers/pydantic.py:31\u001b[0m, in \u001b[0;36mPydanticOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpydantic_object\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     30\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from completion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Failed to parse Suggestions from completion {\"words\": [\"conduct\", \"manner\"]}. Got: 1 validation error for Suggestions\nreasons\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "parser.parse(missformatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed72f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d1b9558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suggestions(words=['conduct', 'manner'], reasons=[\"The word 'conduct' can be used as a substitute for 'behaviour' in the context of how the teacher manages the class.\", \"The word 'manner' can be used to describe the way in which the students behaved in the classroom.\"])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retry_parser.parse_with_prompt(missformatted_output, model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a048a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d10c495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8825e0",
   "metadata": {},
   "source": [
    "#  Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb689b",
   "metadata": {},
   "source": [
    "## __ call __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abfe0ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/llms/openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/charanmannuru/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/llms/openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': 'synthetic'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt_template = \"What is the word to replace the following: {word}\"\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb517d",
   "metadata": {},
   "source": [
    "## Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565e2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "# llm_chain.apply(input_list) # batch processing only for some models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b97729",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b619d7b",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe820ebf",
   "metadata": {},
   "source": [
    "### Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684f5d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air conditioner'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Looking at the context of '{context}'. What is a approapriate word to replace the following; {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=['word', 'context'])\n",
    ")\n",
    "\n",
    "llm_chain.predict(word='fan', context='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36e847f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supporter'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03caa996",
   "metadata": {},
   "source": [
    "### from String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ca733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\"\"\n",
    "llm_chain = LLMChain.from_string(llm=llm, template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa9fae17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air conditioner'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87edd3",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccdd3914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'synthetic, man-made, fake, simulated, imitation, faux, counterfeit, ersatz, fabricated, manufactured'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template= \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[], output_parser=output_parser)\n",
    ")\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1efafbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['synthetic',\n",
       " 'man-made',\n",
       " 'fake',\n",
       " 'simulated',\n",
       " 'imitation',\n",
       " 'faux',\n",
       " 'ersatz',\n",
       " 'fabricated',\n",
       " 'counterfeit',\n",
       " 'pseudo']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict_and_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a04a6e",
   "metadata": {},
   "source": [
    "# Adding Memory Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "668ef7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synthetic', 'man-made', 'simulated']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2a5c34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI: fake', 'imitation', 'faux', 'counterfeit']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict_and_parse(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bccaeeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI: fabricated', 'ersatz', 'pseudo', 'unnatural']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict_and_parse(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b938d934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synthetic',\n",
       " 'man-made',\n",
       " 'simulated',\n",
       " 'fake',\n",
       " 'imitation',\n",
       " 'faux',\n",
       " 'counterfeit',\n",
       " 'fabricated',\n",
       " 'ersatz',\n",
       " 'pseudo',\n",
       " 'unnatural']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict_and_parse(input=\"How may have u given me until now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beafcaa6",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44bda248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/LLM/lib/python3.10/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Answer briefly, write the first 3 options\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['synthetic', 'man-made', 'simulated']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=['history', 'input'], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "conversation.predict_and_parse(input=\"Answer briefly, write the first 3 options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d937dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "Human: Answer briefly, write the first 3 options\n",
      "AI: synthetic, man-made, simulated\n",
      "\n",
      "And the next 4?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imitation', 'fake', 'faux', 'fabricated']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict_and_parse(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5240044",
   "metadata": {},
   "source": [
    "# Sequentail Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746439d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[llm_chain, conversation], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fe03a",
   "metadata": {},
   "source": [
    "# Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10b37d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        # Union of the inpout keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self):\n",
    "        return [\"concat_output\"]\n",
    "    \n",
    "    def _call(self, inputs: Dict[str, str]):\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Output:\n",
      "Artificial means made or produced by human beings rather than occurring naturally. It can also refer to something that is not genuine or real.Synthetic\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(input_variables=['word'],\n",
    "                          template=\" what is the meaning of the following word '{word}'?\",\n",
    "                          )\n",
    "chain_3 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=['word'],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_3, chain_2=chain_2)\n",
    "concat_output = concat_chain.run('artificial')\n",
    "print(f\"Concatenated Output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88530b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Output:\n",
      "Artificial means made or produced by human beings rather than occurring naturally. It can also refer to something that is not genuine or real.Synthetic\n"
     ]
    }
   ],
   "source": [
    "print(f\"Concatenated Output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cfef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

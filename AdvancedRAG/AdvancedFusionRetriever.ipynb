{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0557d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69fdcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "--2025-04-25 18:16:33--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
      "--2025-04-25 18:16:33--  http://arxiv.org/pdf/2307.09288\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  9.45MB/s    in 1.4s    \n",
      "\n",
      "2025-04-25 18:16:35 (9.45 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00cb3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a20f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6001b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", embed_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f5c4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], emded_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524aa6f0",
   "metadata": {},
   "source": [
    "## Step1 : Query generation/ rewriting\n",
    "\n",
    "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n",
    "\n",
    "We can do this by prompting ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb8ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa4243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26185d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryGenPromptStr = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "QueryGenPrompt = PromptTemplate(queryGenPromptStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788f3b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['num_queries', 'query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line, related to the following input query:\\nQuery: {query}\\nQueries:\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryGenPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f41268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQueries(llm, query, numQueries=4):\n",
    "    queryGenPrompt = queryGenPromptStr.format(num_queries=numQueries - 1, query=query)\n",
    "    response = llm.complete(queryGenPrompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fee0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generateQueries(llm, query, numQueries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57704584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Comparison of models developed in this work to open-source chat models in terms of benchmark performance',\n",
       " '2. Evaluation of open-source chat models against models developed in this work using benchmark tests',\n",
       " '3. Analysis of differences between models developed in this work and open-source chat models in benchmark assessments']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ae988",
   "metadata": {},
   "source": [
    "### step 2: Perform vector search for each query \n",
    "Now we run retrieval for each query. This means that we fetch the top-k most relevant results from each vector store.\n",
    "\n",
    "NOTE: We can also have multiple retrievers. Then the total number of queries we run is NM, where N is number of retrievers and M is number of generated queries. Hence there will also be NM retrieved lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffe03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def runQuery(queries, retrivers):\n",
    "\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrivers):\n",
    "            print(i, \"retriver count\")\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "    print(tasks)\n",
    "    taskResults = await tqdm.gather(*tasks)\n",
    "    resultsDict = {}\n",
    "    for i, (query, queryResult) in enumerate(zip(queries, taskResults)):\n",
    "        resultsDict[(query, i)] = queryResult\n",
    "\n",
    "    return resultsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9c9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charanmannuru/miniconda3/envs/llamaIndex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8546a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "[<coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168a25230>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168a252a0>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168a25310>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168a25380>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168a253f0>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168a25460>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await runQuery(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81c3068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[NodeWithScore(node=TextNode(id_='492ad3a7-b1a9-44aa-b219-520131d21a6e', embedding=None, metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '3'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3a269569-d844-44c7-b81e-f67a8f500c5f', node_type='4', metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '3'}, hash='b21f6b597af7703d0e85ce9247c719566bbfd7e16fc4c0750511ef1d1f0dbe9b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 1: Helpfulness human evaluation results for Llama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model generations on ~4k\\nprompts consisting of both single and multi-turn prompts.\\nThe 95% confidence intervals for this evaluation are between\\n1% and 2%. More details in Section 3.4.2. While reviewing\\nthese results, it is important to note that human evaluations\\ncan be noisy due to limitations of the prompt set, subjectivity\\nof the review guidelines, subjectivity of individual raters,\\nand the inherent difficulty of comparing generations.\\nFigure 2: Win-rate % for helpfulness and\\nsafety between commercial-licensed base-\\nlines and Llama 2-Chat, according to GPT-\\n4. To complement the human evaluation, we\\nused a more capable model, not subject to\\nour own guidance. Green area indicates our\\nmodel is better according to GPT-4. To remove\\nties, we used win/(win + loss). The orders in\\nwhich the model responses are presented to\\nGPT-4 are randomly swapped to alleviate bias.\\n1\\nIntroduction\\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\\nchat interfaces, which has led to rapid and widespread adoption among the general public.\\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=3725, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.845077378065503), NodeWithScore(node=TextNode(id_='913efd95-6996-42fd-8f27-ae649a69db96', embedding=None, metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '19'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8de3aa1b-40b3-49b2-b240-2b01ce79c962', node_type='4', metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '19'}, hash='d677595d34b56aea978f4894274e9972107cb2dba47dd03711ba57aa092671b9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models\\nacross ~4,000 helpfulness prompts with three raters per prompt.\\nThe largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\\n36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\\nmodel by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\\nInter-Rater Reliability (IRR).\\nIn our human evaluations, three different annotators provided independent\\nassessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\\nbetter from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\\nthe overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling\\ntasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will\\nbenefit the research community.\\nWe used Gwet’s AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to\\nbe the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness\\ntask that is used in our analysis, Gwet’s AC2 score varies between 0.37 and 0.55 depending on the specific\\nmodel comparison. We see scores on the lower end of that range for ratings from model comparisons with\\nsimilar win rates to each other (like the Llama 2-Chat-70B-chat vs. ChatGPT comparison). We see scores on\\nthe higher end of that range for ratings from model comparisons with a more clear winner (like the Llama\\n2-Chat-34b-chat vs. Falcon-40b-instruct).\\nLimitations of human evaluations.\\nWhile our results indicate that Llama 2-Chat is on par with ChatGPT\\non human evaluations, it is important to note that human evaluations have several limitations.\\n• By academic and research standards, we have a large prompt set of 4k prompts. However, it does not cover\\nreal-world usage of these models, which will likely cover a significantly larger number of use cases.\\n• Diversity of the prompts could be another factor in our results. For example, our prompt set does not\\ninclude any coding- or reasoning-related prompts.\\n• We only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be\\nto ask the models to complete a task and rate the overall experience with the model over multiple turns.\\n• Human evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a\\ndifferent set of prompts or with different instructions could result in different results.\\n19', mimetype='text/plain', start_char_idx=0, end_char_idx=2696, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8428288326338552)], [NodeWithScore(node=TextNode(id_='9531bc2c-dfef-44b4-88b6-5a21a6ff4327', embedding=None, metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '51'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1b8a8303-be39-43d3-b434-fe6e4e6934c3', node_type='4', metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '51'}, hash='76ced0db1ccf81cbe7c527bfac387476d227c9bfc1bbfca81ed0681aa3b5ff22')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Model\\nSize\\nGSM8k\\nMATH\\nMPT\\n7B\\n6.8\\n3.0\\n30B\\n15.2\\n3.1\\nFalcon\\n7B\\n6.8\\n2.3\\n40B\\n19.6\\n5.5\\nLlama 1\\n7B\\n11.0\\n2.9\\n13B\\n17.8\\n3.9\\n33B\\n35.6\\n7.1\\n65B\\n50.9\\n10.6\\nLlama 2\\n7B\\n14.6\\n2.5\\n13B\\n28.7\\n3.9\\n34B\\n42.2\\n6.24\\n70B\\n56.8\\n13.5\\nTable 25: Comparison to other open-source models on mathematical reasoning tasks, GSM8k and MATH\\n(maj1@1 is reported).\\nMathematical Reasoning.\\nIn Table 25, we report results for Llama 2 and other open-source datasets on the\\nGSM8k and MATH tasks.\\nA.3\\nAdditional Details for Fine-tuning\\nA.3.1\\nDetailed Statistics of Meta Human Preference Data\\nTable 26 shows detailed statistics on Meta human preference data. In total, we collected 14 batches of human\\npreference data (i.e., Meta Safety + Helpfulness) on a weekly basis, consisting of over 1 million binary model\\ngeneration comparisons. In general, later batches contain more samples as we onboard more annotators over\\ntime and the annotators also become more familiar with the tasks and thus have better work efficiency. We\\nalso intentionally collect more multi-turn samples to increase the complexity of RLHF data and thus the\\naverage number of tokens per sample also increase accordingly over batches.\\nIn Figure 25, we plot out the preference rating change over batches. It can be clearly seen that the share\\nof samples with similar responses (e.g., negligibly better or unsure) increase dramatically over time while\\nthose with stronger preference (e.g., significantly better) drop in the meantime. This reflects the nature of our\\niterative model update and preference data annotation procedure - with better-performing Llama 2-Chat\\nmodels used for response sampling over time, it becomes challenging for annotators to select a better one\\nfrom two equally high-quality responses.\\nA.3.2\\nCurriculum Strategy for Meta Human Preference Data\\nHigh quality data is critical for alignment as discussed for SFT. We worked closely with the annotation\\nplatforms during our fine-tuning process, and opted for a curriculum annotation strategy. With the first\\nmodel, the annotators were asked to make prompts relatively simple, and then to progressively move towards\\nmore complex prompts and teaching new skills to Llama 2-Chat. An illustration of this curriculum annotation\\non our helpfulness preference data is displayed in Figure 26.\\nA.3.3\\nAblation on Ranking Loss with Preference Rating-based Margin for Reward Modeling\\nWe ablated the ranking loss with the preference rating-based margin term for the helpfulness reward model.\\nWe tried two variants of m(r) with different magnitude for the margin term in Eq 2 as listed open-source 27\\nand compare them against the baseline without the margin term. We report both their per-rating and average\\naccuracy on the Meta Helpful test set in Table 28. We observe that the margin term can indeed help the\\nreward model perform better on more separable comparison pairs and a larger margin can boost it further.\\nHowever, the larger margin also regresses performance on similar samples.\\nWe further evaluated the impact of margin-based loss on reward score distribution shifts. We plot the\\nhistogram of reward scores from the test set in Figure 27. Essentially, the margin term pushes the reward\\n51', mimetype='text/plain', start_char_idx=0, end_char_idx=3177, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=5.20765495300293), NodeWithScore(node=TextNode(id_='492ad3a7-b1a9-44aa-b219-520131d21a6e', embedding=None, metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '3'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3a269569-d844-44c7-b81e-f67a8f500c5f', node_type='4', metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '3'}, hash='b21f6b597af7703d0e85ce9247c719566bbfd7e16fc4c0750511ef1d1f0dbe9b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 1: Helpfulness human evaluation results for Llama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model generations on ~4k\\nprompts consisting of both single and multi-turn prompts.\\nThe 95% confidence intervals for this evaluation are between\\n1% and 2%. More details in Section 3.4.2. While reviewing\\nthese results, it is important to note that human evaluations\\ncan be noisy due to limitations of the prompt set, subjectivity\\nof the review guidelines, subjectivity of individual raters,\\nand the inherent difficulty of comparing generations.\\nFigure 2: Win-rate % for helpfulness and\\nsafety between commercial-licensed base-\\nlines and Llama 2-Chat, according to GPT-\\n4. To complement the human evaluation, we\\nused a more capable model, not subject to\\nour own guidance. Green area indicates our\\nmodel is better according to GPT-4. To remove\\nties, we used win/(win + loss). The orders in\\nwhich the model responses are presented to\\nGPT-4 are randomly swapped to alleviate bias.\\n1\\nIntroduction\\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\\nchat interfaces, which has led to rapid and widespread adoption among the general public.\\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=3725, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=4.600235462188721)], [NodeWithScore(node=TextNode(id_='492ad3a7-b1a9-44aa-b219-520131d21a6e', embedding=None, metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '3'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3a269569-d844-44c7-b81e-f67a8f500c5f', node_type='4', metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '3'}, hash='b21f6b597af7703d0e85ce9247c719566bbfd7e16fc4c0750511ef1d1f0dbe9b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 1: Helpfulness human evaluation results for Llama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model generations on ~4k\\nprompts consisting of both single and multi-turn prompts.\\nThe 95% confidence intervals for this evaluation are between\\n1% and 2%. More details in Section 3.4.2. While reviewing\\nthese results, it is important to note that human evaluations\\ncan be noisy due to limitations of the prompt set, subjectivity\\nof the review guidelines, subjectivity of individual raters,\\nand the inherent difficulty of comparing generations.\\nFigure 2: Win-rate % for helpfulness and\\nsafety between commercial-licensed base-\\nlines and Llama 2-Chat, according to GPT-\\n4. To complement the human evaluation, we\\nused a more capable model, not subject to\\nour own guidance. Green area indicates our\\nmodel is better according to GPT-4. To remove\\nties, we used win/(win + loss). The orders in\\nwhich the model responses are presented to\\nGPT-4 are randomly swapped to alleviate bias.\\n1\\nIntroduction\\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\\nchat interfaces, which has led to rapid and widespread adoption among the general public.\\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nLlama 2-Chat models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=3725, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8406282128580252), NodeWithScore(node=TextNode(id_='913efd95-6996-42fd-8f27-ae649a69db96', embedding=None, metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '19'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8de3aa1b-40b3-49b2-b240-2b01ce79c962', node_type='4', metadata={'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '19'}, hash='d677595d34b56aea978f4894274e9972107cb2dba47dd03711ba57aa092671b9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models\\nacross ~4,000 helpfulness prompts with three raters per prompt.\\nThe largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\\n36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\\nmodel by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\\nInter-Rater Reliability (IRR).\\nIn our human evaluations, three different annotators provided independent\\nassessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\\nbetter from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\\nthe overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling\\ntasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will\\nbenefit the research community.\\nWe used Gwet’s AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to\\nbe the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness\\ntask that is used in our analysis, Gwet’s AC2 score varies between 0.37 and 0.55 depending on the specific\\nmodel comparison. We see scores on the lower end of that range for ratings from model comparisons with\\nsimilar win rates to each other (like the Llama 2-Chat-70B-chat vs. ChatGPT comparison). We see scores on\\nthe higher end of that range for ratings from model comparisons with a more clear winner (like the Llama\\n2-Chat-34b-chat vs. Falcon-40b-instruct).\\nLimitations of human evaluations.\\nWhile our results indicate that Llama 2-Chat is on par with ChatGPT\\non human evaluations, it is important to note that human evaluations have several limitations.\\n• By academic and research standards, we have a large prompt set of 4k prompts. However, it does not cover\\nreal-world usage of these models, which will likely cover a significantly larger number of use cases.\\n• Diversity of the prompts could be another factor in our results. For example, our prompt set does not\\ninclude any coding- or reasoning-related prompts.\\n• We only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be\\nto ask the models to complete a task and rate the overall experience with the model over multiple turns.\\n• Human evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a\\ndifferent set of prompts or with different instructions could result in different results.\\n19', mimetype='text/plain', start_char_idx=0, end_char_idx=2696, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8399700550929777)]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f88ce6",
   "metadata": {},
   "source": [
    "###  Step 3: Perform fusion\n",
    "\n",
    "--> combine results from all the retrievers into one \n",
    "--> remove duplicate that came out along the way\n",
    "--> re rank the nodes based reciprocal rank fusion\n",
    "--> Sumation of 1/ (k+r)\n",
    "--> Reorder nodes by highest to least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ead84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "def fuseResults(resultsDict, similarity_top_k=2):\n",
    "    \n",
    "    k = 60.0\n",
    "    fusedScores = {}\n",
    "    textToNode = {}\n",
    "\n",
    "    # compute the reciprocal rand scores\n",
    "    for nodesWithScore in resultsDict.values():\n",
    "        for rank, nodesWithScore in enumerate(\n",
    "            sorted(\n",
    "                nodesWithScore, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = nodesWithScore.node.get_content()\n",
    "            textToNode[text] = nodesWithScore.node\n",
    "            if text not in fusedScores:\n",
    "                fusedScores[text] = 0.0\n",
    "            fusedScores[text] += 1.0 / (rank + k)\n",
    "    \n",
    "    # sort results\n",
    "    rerankedResults = dict(sorted(fusedScores.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "    rerankedNodes : List[NodeWithScore] = []\n",
    "    for text, score in rerankedResults.items():\n",
    "        rerankedNodes.append(NodeWithScore(node = textToNode[text], score=score))\n",
    "        # print(rerankedNodes[-1])\n",
    "\n",
    "    return rerankedNodes[:similarity_top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "444949b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResults = fuseResults(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb4df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04972677595628415 \n",
      " Figure 1: Helpfulness human evaluation results for Llama\n",
      "2-Chat compared to other open-source and closed-source\n",
      "models. Human raters compared model generations on ~4k\n",
      "prompts consisting of both single and multi-turn prompts.\n",
      "The 95% confidence intervals for this evaluation are between\n",
      "1% and 2%. More details in Section 3.4.2. While reviewing\n",
      "these results, it is important to note that human evaluations\n",
      "can be noisy due to limitations of the prompt set, subjectivity\n",
      "of the review guidelines, subjectivity of individual raters,\n",
      "and the inherent difficulty of comparing generations.\n",
      "Figure 2: Win-rate % for helpfulness and\n",
      "safety between commercial-licensed base-\n",
      "lines and Llama 2-Chat, according to GPT-\n",
      "4. To complement the human evaluation, we\n",
      "used a more capable model, not subject to\n",
      "our own guidance. Green area indicates our\n",
      "model is better according to GPT-4. To remove\n",
      "ties, we used win/(win + loss). The orders in\n",
      "which the model responses are presented to\n",
      "GPT-4 are randomly swapped to alleviate bias.\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\n",
      "complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\n",
      "domains such as programming and creative writing. They enable interaction with humans through intuitive\n",
      "chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\n",
      "methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\n",
      "followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
      "Feedback (RLHF). Although the training methodology is simple, high computational requirements have\n",
      "limited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n",
      "(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\n",
      "match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n",
      "(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\n",
      "as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\n",
      "preferences, which greatly enhances their usability and safety. This step can require significant costs in\n",
      "compute and human annotation, and is often not transparent or easily reproducible, limiting progress within\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\n",
      "annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\n",
      "this paper contributes a thorough description of our fine-tuning methodology and approach to improving\n",
      "LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\n",
      "continue to improve the safety of those models, paving the way for more responsible development of LLMs.\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\n",
      "the emergence of tool usage and temporal organization of knowledge.\n",
      "3 \n",
      "********\n",
      "\n",
      "0.03278688524590164 \n",
      " Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models\n",
      "across ~4,000 helpfulness prompts with three raters per prompt.\n",
      "The largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n",
      "36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\n",
      "model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\n",
      "Inter-Rater Reliability (IRR).\n",
      "In our human evaluations, three different annotators provided independent\n",
      "assessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\n",
      "better from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\n",
      "the overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling\n",
      "tasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will\n",
      "benefit the research community.\n",
      "We used Gwet’s AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to\n",
      "be the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness\n",
      "task that is used in our analysis, Gwet’s AC2 score varies between 0.37 and 0.55 depending on the specific\n",
      "model comparison. We see scores on the lower end of that range for ratings from model comparisons with\n",
      "similar win rates to each other (like the Llama 2-Chat-70B-chat vs. ChatGPT comparison). We see scores on\n",
      "the higher end of that range for ratings from model comparisons with a more clear winner (like the Llama\n",
      "2-Chat-34b-chat vs. Falcon-40b-instruct).\n",
      "Limitations of human evaluations.\n",
      "While our results indicate that Llama 2-Chat is on par with ChatGPT\n",
      "on human evaluations, it is important to note that human evaluations have several limitations.\n",
      "• By academic and research standards, we have a large prompt set of 4k prompts. However, it does not cover\n",
      "real-world usage of these models, which will likely cover a significantly larger number of use cases.\n",
      "• Diversity of the prompts could be another factor in our results. For example, our prompt set does not\n",
      "include any coding- or reasoning-related prompts.\n",
      "• We only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be\n",
      "to ask the models to complete a task and rate the overall experience with the model over multiple turns.\n",
      "• Human evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a\n",
      "different set of prompts or with different instructions could result in different results.\n",
      "19 \n",
      "********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " for n in finalResults:\n",
    "    print(n.score, \"\\n\", n.text, \"\\n********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41829ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

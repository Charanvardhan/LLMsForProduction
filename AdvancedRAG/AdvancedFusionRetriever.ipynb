{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0557d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d69fdcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "--2025-04-28 16:59:34--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.67.42, 151.101.195.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
      "--2025-04-28 16:59:34--  http://arxiv.org/pdf/2307.09288\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  3.16MB/s    in 4.6s    \n",
      "\n",
      "2025-04-28 16:59:39 (2.81 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00cb3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5a20f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6001b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", embed_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f5c4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], emded_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524aa6f0",
   "metadata": {},
   "source": [
    "## Step1 : Query generation/ rewriting\n",
    "\n",
    "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n",
    "\n",
    "We can do this by prompting ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb8ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aaa4243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26185d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryGenPromptStr = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "QueryGenPrompt = PromptTemplate(queryGenPromptStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "788f3b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['num_queries', 'query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line, related to the following input query:\\nQuery: {query}\\nQueries:\\n')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryGenPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72f41268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQueries(llm, query, numQueries=4):\n",
    "    queryGenPrompt = queryGenPromptStr.format(num_queries=numQueries - 1, query=query)\n",
    "    response = llm.complete(queryGenPrompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fee0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generateQueries(llm, query, numQueries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57704584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Comparison of models developed in this work to open-source chat models in terms of benchmark performance',\n",
       " '2. Evaluation of open-source chat models against models developed in this work using benchmark tests',\n",
       " '3. Analysis of differences between models developed in this work and open-source chat models in benchmark testing results']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ae988",
   "metadata": {},
   "source": [
    "### step 2: Perform vector search for each query \n",
    "Now we run retrieval for each query. This means that we fetch the top-k most relevant results from each vector store.\n",
    "\n",
    "NOTE: We can also have multiple retrievers. Then the total number of queries we run is NM, where N is number of retrievers and M is number of generated queries. Hence there will also be NM retrieved lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dffe03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def runQuery(queries, retrivers):\n",
    "\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrivers):\n",
    "            print(i, \"retriver count\")\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "    print(tasks)\n",
    "    taskResults = await tqdm.gather(*tasks)\n",
    "    resultsDict = {}\n",
    "    for i, (query, queryResult) in enumerate(zip(queries, taskResults)):\n",
    "        resultsDict[(query, i)] = queryResult\n",
    "\n",
    "    return resultsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed9c9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8546a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "[<coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1685e6f80>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1685e6340>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1685e6ff0>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1685e7060>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1685e7140>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1685e71b0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await runQuery(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81c3068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. Comparison of models developed in this work to open-source chat models in terms of benchmark performance', 0)\n",
      "Node ID: 9ef63423-c0a7-408e-959d-bca06160b455\n",
      "Text: Figure 1: Helpfulness human evaluation results for Llama 2-Chat\n",
      "compared to other open-source and closed-source models. Human raters\n",
      "compared model generations on ~4k prompts consisting of both single\n",
      "and multi-turn prompts. The 95% confidence intervals for this\n",
      "evaluation are between 1% and 2%. More details in Section 3.4.2. While\n",
      "reviewing the...\n",
      "Score:  0.845\n",
      "\n",
      "Node ID: 95ecc7d2-594f-4121-bcf9-424c906374d6\n",
      "Text: Figure 12: Human evaluation results for Llama 2-Chat models\n",
      "compared to open- and closed-source models across ~4,000 helpfulness\n",
      "prompts with three raters per prompt. The largest Llama 2-Chat model\n",
      "is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n",
      "36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B\n",
      "model outperf...\n",
      "Score:  0.843\n",
      "\n",
      "***************\n",
      "('2. Evaluation of open-source chat models against models developed in this work using benchmark tests', 1)\n",
      "Node ID: 3185c876-2591-42a3-8652-97224140db3e\n",
      "Text: Model Size GSM8k MATH MPT 7B 6.8 3.0 30B 15.2 3.1 Falcon 7B 6.8\n",
      "2.3 40B 19.6 5.5 Llama 1 7B 11.0 2.9 13B 17.8 3.9 33B 35.6 7.1 65B\n",
      "50.9 10.6 Llama 2 7B 14.6 2.5 13B 28.7 3.9 34B 42.2 6.24 70B 56.8 13.5\n",
      "Table 25: Comparison to other open-source models on mathematical\n",
      "reasoning tasks, GSM8k and MATH (maj1@1 is reported). Mathematical\n",
      "Reasoning. In...\n",
      "Score:  5.208\n",
      "\n",
      "Node ID: 9ef63423-c0a7-408e-959d-bca06160b455\n",
      "Text: Figure 1: Helpfulness human evaluation results for Llama 2-Chat\n",
      "compared to other open-source and closed-source models. Human raters\n",
      "compared model generations on ~4k prompts consisting of both single\n",
      "and multi-turn prompts. The 95% confidence intervals for this\n",
      "evaluation are between 1% and 2%. More details in Section 3.4.2. While\n",
      "reviewing the...\n",
      "Score:  4.600\n",
      "\n",
      "***************\n",
      "('3. Analysis of differences between models developed in this work and open-source chat models in benchmark testing results', 2)\n",
      "Node ID: 9ef63423-c0a7-408e-959d-bca06160b455\n",
      "Text: Figure 1: Helpfulness human evaluation results for Llama 2-Chat\n",
      "compared to other open-source and closed-source models. Human raters\n",
      "compared model generations on ~4k prompts consisting of both single\n",
      "and multi-turn prompts. The 95% confidence intervals for this\n",
      "evaluation are between 1% and 2%. More details in Section 3.4.2. While\n",
      "reviewing the...\n",
      "Score:  0.841\n",
      "\n",
      "Node ID: 95ecc7d2-594f-4121-bcf9-424c906374d6\n",
      "Text: Figure 12: Human evaluation results for Llama 2-Chat models\n",
      "compared to open- and closed-source models across ~4,000 helpfulness\n",
      "prompts with three raters per prompt. The largest Llama 2-Chat model\n",
      "is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n",
      "36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B\n",
      "model outperf...\n",
      "Score:  0.840\n",
      "\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for i,j in results_dict.items():\n",
    "    print(i)\n",
    "    for k in j:\n",
    "        print(k)\n",
    "    print(\"***************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f88ce6",
   "metadata": {},
   "source": [
    "###  Step 3: Perform fusion\n",
    "\n",
    "--> combine results from all the retrievers into one \n",
    "--> remove duplicate that came out along the way\n",
    "--> re rank the nodes based reciprocal rank fusion\n",
    "--> Sumation of 1/ (k+r)\n",
    "--> Reorder nodes by highest to least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ead84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "def fuseResults(resultsDict, similarity_top_k=2):\n",
    "    \n",
    "    k = 60.0\n",
    "    fusedScores = {}\n",
    "    textToNode = {}\n",
    "\n",
    "    # compute the reciprocal rand scores\n",
    "    for nodesWithScore in resultsDict.values():\n",
    "        for rank, nodesWithScore in enumerate(\n",
    "            sorted(\n",
    "                nodesWithScore, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = nodesWithScore.node.get_content()\n",
    "            textToNode[text] = nodesWithScore.node\n",
    "            if text not in fusedScores:\n",
    "                fusedScores[text] = 0.0\n",
    "            fusedScores[text] += 1.0 / (rank + k)\n",
    "    \n",
    "    # sort results\n",
    "    rerankedResults = dict(sorted(fusedScores.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "    rerankedNodes : List[NodeWithScore] = []\n",
    "    for text, score in rerankedResults.items():\n",
    "        rerankedNodes.append(NodeWithScore(node = textToNode[text], score=score))\n",
    "        # print(rerankedNodes[-1])\n",
    "\n",
    "    return rerankedNodes[:similarity_top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "444949b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResults = fuseResults(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04972677595628415 \n",
      " Figure 1: Helpfulness human evaluation results for Llama\n",
      "2-Chat compared to other open-source and closed-source\n",
      "models. Human raters compared model generations on ~4k\n",
      "prompts consisting of both single and multi-turn prompts.\n",
      "The 95% confidence intervals for this evaluation are between\n",
      "1% and 2%. More details in Section 3.4.2. While reviewing\n",
      "these results, it is important to note that human evaluations\n",
      "can be noisy due to limitations of the prompt set, subjectivity\n",
      "of the review guidelines, subjectivity of individual raters,\n",
      "and the inherent difficulty of comparing generations.\n",
      "Figure 2: Win-rate % for helpfulness and\n",
      "safety between commercial-licensed base-\n",
      "lines and Llama 2-Chat, according to GPT-\n",
      "4. To complement the human evaluation, we\n",
      "used a more capable model, not subject to\n",
      "our own guidance. Green area indicates our\n",
      "model is better according to GPT-4. To remove\n",
      "ties, we used win/(win + loss). The orders in\n",
      "which the model responses are presented to\n",
      "GPT-4 are randomly swapped to alleviate bias.\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\n",
      "complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\n",
      "domains such as programming and creative writing. They enable interaction with humans through intuitive\n",
      "chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\n",
      "methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\n",
      "followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
      "Feedback (RLHF). Although the training methodology is simple, high computational requirements have\n",
      "limited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n",
      "(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\n",
      "match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n",
      "(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\n",
      "as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\n",
      "preferences, which greatly enhances their usability and safety. This step can require significant costs in\n",
      "compute and human annotation, and is often not transparent or easily reproducible, limiting progress within\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\n",
      "annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\n",
      "this paper contributes a thorough description of our fine-tuning methodology and approach to improving\n",
      "LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\n",
      "continue to improve the safety of those models, paving the way for more responsible development of LLMs.\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\n",
      "the emergence of tool usage and temporal organization of knowledge.\n",
      "3 \n",
      "********\n",
      "\n",
      "0.03278688524590164 \n",
      " Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models\n",
      "across ~4,000 helpfulness prompts with three raters per prompt.\n",
      "The largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n",
      "36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\n",
      "model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\n",
      "Inter-Rater Reliability (IRR).\n",
      "In our human evaluations, three different annotators provided independent\n",
      "assessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\n",
      "better from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\n",
      "the overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling\n",
      "tasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will\n",
      "benefit the research community.\n",
      "We used Gwet’s AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to\n",
      "be the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness\n",
      "task that is used in our analysis, Gwet’s AC2 score varies between 0.37 and 0.55 depending on the specific\n",
      "model comparison. We see scores on the lower end of that range for ratings from model comparisons with\n",
      "similar win rates to each other (like the Llama 2-Chat-70B-chat vs. ChatGPT comparison). We see scores on\n",
      "the higher end of that range for ratings from model comparisons with a more clear winner (like the Llama\n",
      "2-Chat-34b-chat vs. Falcon-40b-instruct).\n",
      "Limitations of human evaluations.\n",
      "While our results indicate that Llama 2-Chat is on par with ChatGPT\n",
      "on human evaluations, it is important to note that human evaluations have several limitations.\n",
      "• By academic and research standards, we have a large prompt set of 4k prompts. However, it does not cover\n",
      "real-world usage of these models, which will likely cover a significantly larger number of use cases.\n",
      "• Diversity of the prompts could be another factor in our results. For example, our prompt set does not\n",
      "include any coding- or reasoning-related prompts.\n",
      "• We only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be\n",
      "to ask the models to complete a task and rate the overall experience with the model over multiple turns.\n",
      "• Human evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a\n",
      "different set of prompts or with different instructions could result in different results.\n",
      "19 \n",
      "********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for n in finalResults:\n",
    "   print(n.score, \"\\n\", n.text, \"\\n********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fa9b487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e2d7f",
   "metadata": {},
   "source": [
    "## Lets plug this into a retriver query engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41829ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "\n",
    "class FusedRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, retrievers: List[BaseRetriever], similarity_top_k=2):\n",
    "        self._llm = llm\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "\n",
    "        queries = generateQueries(self._llm, query_bundle.query_str, numQueries=4)\n",
    "        results = asyncio.run(runQuery(queries, self._retrievers))\n",
    "\n",
    "        finalResults = fuseResults(results, similarity_top_k=self._similarity_top_k)\n",
    "        return finalResults\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44d481b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "fusionRetriever = FusedRetriever(llm, [vector_retriever, bm25_retriever], similarity_top_k=2)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(fusionRetriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f199c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "[<coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1686810e0>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168681150>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x1686811c0>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168681070>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168681230>, <coroutine object Dispatcher.span.<locals>.async_wrapper at 0x168681310>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.70it/s]\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d849dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The models developed in this work generally perform better than existing open-source models based on the helpfulness and safety benchmarks tested. They also appear to be on par with some of the closed-source models, at least according to the human evaluations conducted.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

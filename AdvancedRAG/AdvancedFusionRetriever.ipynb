{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0557d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fdcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00cb3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a20f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6001b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", embed_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5c4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], emded_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524aa6f0",
   "metadata": {},
   "source": [
    "## Step1 : Query generation/ rewriting\n",
    "\n",
    "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries.\n",
    "\n",
    "We can do this by prompting ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb8ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26185d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryGenPromptStr = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "QueryGenPrompt = PromptTemplate(queryGenPromptStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788f3b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['num_queries', 'query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line, related to the following input query:\\nQuery: {query}\\nQueries:\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryGenPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f41268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQueries(llm, query, numQueries=4):\n",
    "    queryGenPrompt = queryGenPromptStr.format(num_queries=numQueries - 1, query=query)\n",
    "    response = llm.complete(queryGenPrompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fee0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = generateQueries(llm, query, numQueries=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57704584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Comparison of models developed in this work to open-source chat models in terms of benchmark performance',\n",
       " '2. Evaluation of open-source chat models against models developed in this work using benchmark tests',\n",
       " '3. Analysis of differences between models developed in this work and existing open-source chat models in benchmark evaluations']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ae988",
   "metadata": {},
   "source": [
    "### step 2: Perform vector search for each query \n",
    "Now we run retrieval for each query. This means that we fetch the top-k most relevant results from each vector store.\n",
    "\n",
    "NOTE: We can also have multiple retrievers. Then the total number of queries we run is NM, where N is number of retrievers and M is number of generated queries. Hence there will also be NM retrieved lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dffe03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def runQuery(queries, retrivers):\n",
    "\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrivers):\n",
    "            print(i, \"retriver count\")\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    taskResults = await tqdm.gather(*tasks)\n",
    "    resultsDict = {}\n",
    "    for i, (query, queryResult) in enumerate(zip(queries, taskResults)):\n",
    "        resultsDict[(query, i)] = queryResult\n",
    "\n",
    "    return resultsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed9c9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8546a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n",
      "0 retriver count\n",
      "1 retriver count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await runQuery(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ead84d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "print(len(index.docstore.get_all_ref_doc_info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444949b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaIndex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
